# 大规模性能架构特征标志|线束

> 原文：<https://www.harness.io/blog/architecting-feature-flags-performance>

了解 Harness 如何创建即使在最大规模下也能发挥作用的功能标志管理解决方案。

工程师真正擅长的两件事是什么？如果你猜是构建事物和优化事物，你完全正确。然而，更重要的是，它在构建和优化任何规模的性能。对于最终推向客户的任何应用程序或服务，工程团队都非常关注交付高性能的代码，以便用户体验是一致的和令人愉快的。

线束特征标志也是如此。仅仅构建一个特性标志解决方案是不够的，它还需要能够处理最大用例的负载，这可能涉及**数百万用户。**我们还讨论了为开发人员构建的利用特性标志——除了易用性之外，哪种开发人员工具不考虑大规模性能？

今天，我们想展示我们自己构建可扩展和高性能特征标志解决方案的历程——从我们的 MVP 开始，到我们的最终产品供客户在生产中使用。另外，我们想分享一些我们一路走来学到的经验。

## 最初的模型

特征标志系统基本上有两个主要部分。一个是我们所说的特性标志 SDK，它嵌入在目标应用程序中，将与特性标志服务器对话。SDK 从服务器接收功能标志更新。

其次，将会有一个 UI/API 组件，它将接受输入来对特性标志进行更改。一旦这些变化进入系统，我们希望这些变化直接流向 SDK，就像推送通知一样。因此，当我们在 UI 上得到更改时，我们希望向 SDK 发送一个推送通知，通知功能标志的更改，然后我们希望在应用程序中进行适当的更改。

还有另一种机制——投票模型。考虑一下订户可能断开网络连接的情况——我们希望通过基于轮询的模型积极主动地发现这种情况。我们还希望支持这种机制，以便从服务器和系统获得支持这两种模型所需的功能更新。这是我们出发的地方。

## 构建核心系统

我们开始用 Golang 开发我们的应用程序。首先，我们选择 Golang 有两个原因:它的性能非常好，速度也很快；而且开发速度非常快(比 Java 之类的东西快得多，Java 是开发应用程序的传统企业平台)。

显然，我们需要选择一个数据库。我们使用关系数据库，我们使用 Postgres 主要是因为它非常受欢迎，它有很多扩展，并且它还将帮助我们构建度量服务，我们将在后面介绍。在这方面，我们从一个非常简单的分层应用程序开始。

我们从一个 UI 组件开始，它实际上是跨 Harness 平台共享的。现在我们有了 UI 服务，它作为一个单独的服务运行，我们有了功能标志服务器，它为我们的功能标志组件提供服务。这个特定的服务器从 UI 输入中获取负载，并向下游传输到 SDK。现在，实际使用 UI 服务器或 UI 服务并对特性标志进行更改的用户数量将会相当少。使用这些 SDK 的应用程序的数量以及这些更改将流向的用户将会非常多。

一个简单的例子是，你有一个开发人员、DevOps 人员或产品经理，他们将控制功能标志，这些变化可能会流向成千上万的 iPhones 或 Android 设备。因此，我们需要将这项服务分解为两个组件，这两个组件可以独立地解决和扩展这个系统。

管理服务将只负责从 UI 或 API 获取输入，其中将包含我们正在尝试进行的所有功能标志更改。然后，这些更改将被传送到特性标志 SDK 服务，SDK 服务将负责与下游的所有 SDK 进行通信。因此，我们将服务分解为两个组件，这允许我们独立地扩展这两个服务。

我们有管理服务，这可能是一个较低数量的容器，因为它不会为大量用户提供服务。SDK 服务可以独立扩展，为大量用户提供服务。这是我们为解决这个特殊用例而修改的第一个设计决策。

展望未来，我们意识到组件的数量，SDK 的数量，实际上仍然会相当大。我们仍然需要大量的 SDK 服务容器来服务所有的 SDK。这又一次不起作用。我们进行了调查，发现我们需要另一个组件来负责通信和维护与客户端的网络连接。

我们需要第一种机制来维护与所有 SDK 的持久连接，以便下推通知，为此，我们在 SDK 和 SDK 服务之间引入了另一个组件。我们利用了一个叫做图钉的开源库。这是一个非常棒的库，专门维护不同 SDK 的服务器端连接。图钉组件是一个有趣的组件，它实际上为我们提供了大约 50 倍或 100 倍的可伸缩性，而没有实际增加我们的基础架构成本。每个图钉服务都能够通过标准 HTTP 协议维护数万个并发 SSE(服务器端事件)客户端连接，这意味着我们不需要为客户端利用任何额外的库。反过来，每个图钉实例维护到 SDK 服务容器的单个连接。从应用程序的角度来看，对于如何管理 SSE 连接的所有知识没有特殊的技巧要求。

当用户界面发生变化时，想想工作流。UI 服务调用管理服务，管理服务对数据库进行更改，数据库通知 SDK 服务，然后 SDK 服务将流式更新发送到图钉服务。图钉服务知道所有的连接和 SDK 通道，它向所有感兴趣的 SSE 通道发送通知，这意味着客户端将会收到通知。

客户端对 SDK 服务进行 HTTP 调用，它将从 SDK 服务获取最新的配置。然后，SDK 将更新本地缓存。最后，应用程序将获得最新的特性配置。这真的很有效。

## 我们如何衡量这一点？

开始的时候我们说推送通知很高明。但是，当 SDK 向 SDK 服务查询最新配置时，我们看到大量请求涌入数据库。事实上，这给系统制造了一个瓶颈。我们如何优化这一点？是时候引入缓存层了。

对此，显而易见的选择是引入 Redis 缓存。现在，SDK 服务将在 Postgres 数据库之前访问 Redis 缓存，而不是直接查询数据库。如果数据在 Postgres 中，它将获取数据，预先填充将从所有表中构建的特性标志配置，然后将更新后的配置存储在 Redis 中。这当然减少了数据库查询的数量，并释放了数据库的负载。这对我们帮助很大。

优化的下一部分是我们如何扩展图钉。Pushpin 实际上是一种无状态服务，push 本身并不知道它必须将数据发送到哪些连接，以及不发送数据到哪些连接。我们需要一种机制来将适当的事件更新发送到所有的图钉服务器，如果没有这种机制，就无法将图钉扩展到多个实例。这实际上是必须的:缩放图钉，这样整个系统将成为一个水平可缩放的系统。

为了解决这个问题，我们引入了另一层:消息传递层。因为我们已经将 Redis 作为基础架构的一部分，所以我们决定评估 Redis 流，这是一种基于 Redis 的发布/订阅机制，与 Redis 发布/订阅不同，但性能更好。我们的想法是使用 Redis 流，这样我们就可以将图钉实例的数量扩展到一个以上。因此，向 SDK 交付更新的新机制是，每当我们在 UI 上进行任何更改时，管理服务就可以通过 Redis 流向 SDK 发送通知。

我们将这一变化引入到我们的架构中，因为我们利用了一个消息传递系统，该系统允许我们拥有一个基于消息传递的架构和事件驱动的机制，以便在我们不同的微服务之间进行通信。我们开始使用 Redis 流在管理和 SDK 服务之间进行通信，还开始使用相同的注册中心名称与不同的图钉实例进行下游通信。

现在，每个图钉实例都向 SDK 服务注册，我们知道图钉服务对哪些通道感兴趣，并且通道携带每个 SDK 实例已经注册的环境 ID。每个客户端将使用环境 ID 订阅 SSE 通道，反过来，这些将是图钉实例向不同的 SDK 容器注册的主题。

一旦进行了功能标志更新，这些更改就会传播到 SDK 服务，然后 SDK 服务会在它们唯一的环境 ID 上发送通知。每个图钉实例都知道正在对哪个环境 ID 进行更改，并向下游发送到具有相同环境 ID 的所有不同通道。

假设在一个实例中，一个图钉实例可以访问 200 个具有相同 ID 的特性标志 SDK。它将向这 200 个 SDK 发送推送通知；另一个实例没有连接到任何匹配该 ID 的 SDK，它将忽略该消息。这样，我们可以将 Pushpin 横向扩展为一个非常轻量级的微服务。一个好处是，我们可以在不增加基础设施成本的情况下多路复用系统的整个连接，并帮助我们减少系统的整体延迟。从我们开始的地方到我们当时到达的地方，这是一个很大的进步。

## 指标收集简介

我们想介绍的下一件事是从我们的 SDK 收集所有指标。服务器端 SDK 做了大量评估，从客户端 SDK，我们需要收集大量关于标志使用的数据。当然，必须聚合和收集这些数据，然后我们必须在这些数据的基础上构建智能分析。为此，我们必须构建一个完整的指标服务。

为了初始化并启动这个度量服务，我们必须从对所有 SDK 的两个改进开始。对于所有 SDK，我们开始收集给定应用程序的所有特性标志使用情况以及对 SDK 本身的评估。我们预先聚合了其他特性标志使用的数据，然后每隔一段时间，我们需要将这些数据传输到度量服务。

我们知道这项指标服务将是一项非常数据密集型的服务，我们必须考虑两个部分。一个是数据接收:我们希望确保接收到的数据不会导致 SDK 本身变慢。例如，如果度量服务关闭，它不应该阻止特性标志 SDK 执行或在系统上添加额外的负载。其次，我们将从所有 SDK 中收集大量数据，这需要有用，同时还要保持高性能。作为可扩展系统的一部分，它需要近乎实时地可用。

我们要做的第一件事是选择一个合适的数据库，它将能够发布、处理和汇总这些数据。我们使用了一个基于时间序列的数据库，名为 TimescaleDB。TimescaleDB 是一个非常有趣的数据库。它基于 Postgres，因此它为我们的基础架构提供了统一性，并提供了灵活性和协同作用，使我们的功能标志系统也能在本地部署，这是我们未来的目标之一。

总之，TimescaleDB 是一个时间序列数据库，专门存储大量的时间序列数据，具有水平可伸缩性和高性能。这对于我们的用例来说非常好，我们有一个基于时间序列的视图，可以看到对于任何过滤标准，功能标志的使用在一段时间内是如何发展的。

对于这个度量服务，最明显的事情是将它分成两个部分，摄取服务和处理服务。摄取服务位于网络的最边缘，它的工作非常简单:获取从不同 SDK 收集的数据，并将其传递到下游进行处理。

指标处理服务更像是一个异步流程，它使用来自摄取服务的数据，并将其发送到我们的分析数据库进行处理存储。我们已经有了一个消息传递系统 Redis Streams，所以我们决定利用这个基础设施。我们所做的唯一改变是，我们决定给它一个专用的 Redis 流实例，而不是使用同一个 Redis 流实例。这对我们来说是非常重要的，主要原因和我之前提到的一样。

我们用于主要系统的 Redis 流实例是我们基础架构的重要组成部分，用于解决我们的功能标记使用问题，不应该干扰我们的分析数据库。我们的 analytics DB 和 Metrics 服务更像是我们系统的次要部分，这一点很重要，但如果系统的任何一个部分出现故障，这些独立的服务可以增强恢复能力。这就是我们想要实现的目标。这就是为什么我们把这两个分解成拥有自己专用的 Redis 流实例:这样每个实例的负载就不会影响另一个实例。

## 网络、计算和当前状态

我们想要涵盖的最后一部分是我们如何组织我们的网络、我们的计算，以及从这个角度来看事情的现状。到目前为止，我描述的整个服务——这是我们的功能标志服务器的一部分，它仍然是 Harness 平台的一部分。这些是专用于功能标志的不同微服务。但这是由不同组件组成的 Harness 平台的子集。

我们有 CD、CI、功能标志和云成本管理，所有这些都整合到一个平台中。我们还利用其他 Harness 平台微服务并与之交流。我不会深入细节，因为这是一个不同的范围，但是可以把它想象成在我刚刚描述的特性标志之外还有一个完整的世界。

整个系统位于我们的 Kubernetes 集群中，所有这些微服务都是不同的单元。我们使用 GCLB 负载平衡器，它位于我们网络的边缘，用于基于图钉的出站系统和指标摄取服务。这个模型给了我们安全感、可伸缩性和安心感，即无论我们试图实现什么用例，系统都是安全的、可伸缩的。

我想强调的另一件事是，这些组件，如 Redis 流、TimescaleDB、数据库和缓存层，都像是基于即插即用的基础设施。根据需求、部署策略、SaaS 或本地部署，这些都可以替换为我们想要支持的任何基础架构。

## 最后的想法

这涵盖了功能标志架构如何发展的平台演示、目前的架构是什么，以及我们在设计该架构时如何解决可扩展性和延迟问题，并且考虑到我们所拥有的设计灵活性，还介绍了未来的发展可能性。谢谢您们。

如果你想看看特色标志的魔力，[开始免费试用](https://app.harness.io/auth/#/signup/?module=cf)或[今天就请求演示](https://harness.io/demo/)。