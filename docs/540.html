<html>
<head>
<title>Auto-Pruning Orphaned Resources With Harness Kubernetes Deployments | Harness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>使用 Harness Kubernetes 部署| Harness 自动修剪孤立资源</h1>
<blockquote>原文：<a href="https://www.harness.io/blog/auto-pruning-orphaned-resources#0001-01-01">https://www.harness.io/blog/auto-pruning-orphaned-resources#0001-01-01</a></blockquote><div><p>我们的自动修剪功能处理从清单中删除但仍在集群上的资源。向清理工作说再见！</p><div fs-richtext-element="rich-text" class="blog-rtf w-richtext"><p>没有人希望在他们的部署环境中悬挂资源。它们增加了混乱和不必要的资源消耗。当环境由拥有分布式所有权和知识的多个管理员/开发人员管理时，这尤其令人痛苦。此外，当使用高级部署策略，如<a href="https://harness.io/blog/blue-green-canary-deployment-strategies/" target="_blank"> canary 和 blue-green </a>时，跟踪这些资源变得更加复杂，尤其是在处理版本回滚时。为了将你从上述情况中解救出来，我们在 Harness 提供了一个受控的、可定制的解决方案，叫做自动修剪。这意味着你可以安心地专注于更重要的任务。</p><p>我们让你兴奋了吗？请继续阅读，了解更多关于我们的方法和设计的信息。</p><h2><strong>带线束的日常 K8s 部署</strong></h2><figure class="w-richtext-figure-type- "><figcaption>Release R1 - Successful Deployment</figcaption></figure><p>在讨论修剪之前，我们必须了解一个带有 Harness 的常规 K8s 部署。我们支持多种部署策略，如滚动、金丝雀和蓝绿色。此外，您可以从多个清单源中进行选择。让我们通过一个简单的滚动部署示例来深入探讨这一点。</p><p>首先，您必须<a href="https://docs.harness.io/article/2j2vi5oxrq-define-kubernetes-manifests#step_1_create_the_harness_kubernetes_service" target="_blank">创建一个服务</a>。然后，通过提供各自的连接器信息，选择您想要部署的清单源和工件。</p><p>接下来，在<a href="https://docs.harness.io/article/u3rp89v80h-define-your-kubernetes-target-infrastructure" target="_blank">基础设施定义部分</a>中定义您的目标基础设施，比如集群和名称空间。</p><p>最后，使用您在前面的步骤中刚刚创建的服务和基础设施定义创建一个具有滚动部署策略的工作流，并运行该工作流以在您的环境中进行部署。</p><p>就是这样！连续发货快乐！</p><h2><strong>孤儿资源问题</strong></h2><p>在组织层面，这些部署的频率和复杂性都很高。这意味着必须在 Kubernetes 集群上设置不同的环境来推出或测试不同的版本。这就是为什么团队必须经常更新他们的清单。所有这些加起来就是集群上的一行无主或未跟踪的资源，这些资源在各种更新期间从清单中删除，但现在不必要地出现在集群上。除了未被跟踪之外，这些资源还可能导致意外的行为。</p><p>我们的一个客户面临这种情况——他们的集群中有一个不希望的入口，这导致了不正确的路由。对我们的许多客户来说，手动管理和清理这些资源一直是个问题。</p><p>因此，我们推出了自动修剪功能来处理这些问题，并改善 Kubernetes 的体验。</p><h2><strong>分解问题</strong></h2><p>回到我们在环境中悬而未决的资源问题，我们将各个方面细分如下:</p><ul role="list"><li>用不同的策略决定修剪和处理回滚的过滤标准。</li><li>哪里，什么，如何</li><li>存储和安全问题</li></ul><p>在下一节中，我们将讨论针对上述每个方面的设计。</p><h2><strong>用不同的策略决定修剪和处理回滚的过滤标准</strong></h2><h3>滚动部署案例 A</h3><p>比方说，在第一个版本(R1)中，您从清单(M1)中部署资源(A，B，C)。</p><p>接下来，在第二个版本(R2)中，您将您的清单更新到版本 M2，它现在包含资源 A、B 和 D，然后您部署它。</p><p>现在，在生产环境中，您有资源 A、B、C 和 D，其中 C 是不需要的资源。</p><figure class="w-richtext-figure-type- "><figcaption>Release R2 Case A - Auto-Pruning Post Successful Deployment</figcaption></figure><h3>滚动部署案例 B</h3><p>对上述场景的修改是，假设 R2 发布失败，生产环境必须回滚到 R1 发布部署后的状态。在这种情况下，我们必须处理在部署新版本 R2 时被删除的资源的重新创建(在我们的例子中，是资源 C)。</p><p>这里值得一提的另一点是，通过回滚，我们将环境恢复到上次成功部署后的状态。因此，假设在您当前和上一次成功部署之间部署了多个失败的版本。然后，回滚后，您的环境将恢复到上次成功部署后的状态。</p><figure class="w-richtext-figure-type- "><figcaption>Release R2 Case B - Rollback After Failed Deployment</figcaption></figure><h3>蓝绿色部署</h3><p>蓝绿策略有点不一样。这就是为什么它需要不同的处理来实现自动修剪。一个示例场景如下:</p><p>假设您已经先后部署了两个版本，R1 和 R2。R1 包含 M1 清单中定义的资源 A、B 和 C，R2 包含 M2 清单中定义的资源 A、B 和 D。目前，您的发布 R1 与阶段设置相关联，而发布 R2 与生产设置相关联。因此，您的集群包含资源 A、B、C 和 d。</p><p>接下来，将您的清单更新为 M3，其中包含资源 A、B 和 E，并且您希望将这个新版本 R3 部署为 prod setup。因此，在部署完成后，您的发布 R2 将与 stage setup 关联，而 R3 将与 prod 关联。现在，您的集群包含资源 A、B、C、D 和 E。</p><p>在上面的场景中，资源 C 是不需要的。如果以一种一般化的方式重新表述，那么在与先前阶段设置相关联的版本中明确存在的资源将成为删减的候选。</p><figure class="w-richtext-figure-type- "><figcaption>Auto-Pruning Post Blue-Green Deployment</figcaption></figure><h2><strong>地点、内容和方式</strong></h2><p>Harness 将发布历史存储在集群上的配置映射发布历史中。这包含一些关于工作负载、CRD 和版本信息的元数据。在任何时间点，它包含两个部署的状态:当前和上次成功的部署。</p><p>出于修剪的目的，我们使用这个配置图。当部署以新的清单 M_NEW 开始时，我们将在最后一个成功发布中部署的资源与 M_NEW 中的资源进行比较。在最后一个成功的版本中的资源和当前版本中没有的资源在部署后都会被删除。</p><p><strong>回滚中的重建</strong>:我们还将资源的渲染 YAML 保存为 ConfigMap 中的一个字符串。在回滚发生的情况下，这就是我们如何在当前部署步骤中从该 YAML 重新创建已经删除的资源。</p><p><strong>可定制性</strong>:可能会有一个场景，你不希望某些特定的资源被包括进来进行修剪。为了解决这个问题，我们提供了一个注释，<strong>harness.io/skipPruning:真</strong>，您可以将它包含在您的资源 YAML 中。</p><p><strong>处理干预</strong>:在您的环境中可能有多种干预会导致自动修剪失败。这可能包括手动删除应该在当前部署中删除的资源或无法访问的集群等情况。在这些情况下，我们让管道继续执行进一步的步骤，而不是在那里失败..</p><h2><strong>存储和安全</strong></h2><p>清单大小通常大于配置映射存储限制，即 1 Mb。这就是为什么我们在将呈现的清单保存到 ReleaseHistory ConfigMap 之前，使用广泛接受的具有 BEST_COMPRESSION 设置的 Java 紧缩器对其进行压缩和编码。这解决了我们存储大于 1 Mb 的清单的问题。</p><p>我们面临的另一个问题是，客户不希望公开他们的 CRD。这就是为什么我们将 CRD 的渲染 YAML 保存为 Kubernetes Secrets 而不是 ConfigMap。</p><h2><strong>结论</strong></h2><p>许多客户已经在使用我们的自动修剪功能，节省了保持环境清洁的额外工作。因此，他们节省了跟踪和清理这些资源的手动工作。此外，这还有助于他们根据清单更新配置，而不必担心由于孤立资源而导致的任何意外行为。</p><p>为了利用自动修剪，客户不再局限于使用原生 Helm，它仅在基本部署策略中可用。相反，他们现在可以选择 Kubernetes 部署，在那里他们可以将它与高级部署策略一起使用，如 canary 和 blue-green，以及多个清单存储的选项。</p><p>我们希望您喜欢这篇关于使用 Harness 改善 Kubernetes 体验的文章。那么，你还在等什么？跳上金丝雀船长，带着<a href="https://app.harness.io/auth/#/signup" target="_blank">挽具</a>开始你的旅程。</p><p>如果你还没有准备好，继续阅读和学习更多！我们已经几次提到金丝雀和蓝绿色的发布策略，那么为什么不多熟悉一下这些概念呢？现在就阅读我们的文章:<a href="https://harness.io/blog/blue-green-canary-deployment-strategies/" target="_blank">介绍部署策略:蓝绿色、淡黄色等等</a>。</p></div></div>    
</body>
</html>