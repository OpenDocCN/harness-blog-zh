<html>
<head>
<title>Resource Optimization of Harness CD Community Edition - Part 1 | Harness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>线束光盘社区版的资源优化-第1部分|线束</h1>
<blockquote>原文：<a href="https://www.harness.io/blog/community-edition-resources-part1#0001-01-01">https://www.harness.io/blog/community-edition-resources-part1#0001-01-01</a></blockquote><div><p>了解我们如何在3 GB内存和2个CPU的资源要求下提供Harness CD Community Edition。</p><div fs-richtext-element="rich-text" class="blog-rtf w-richtext"><p>最近，Harness推出了它的CD社区版。这是Harness CD的一个永久免费、有源代码的本地版本。加入我们的旅程，了解我们如何设法将Harness CD Community Edition与3 GB内存和2个CPU的资源要求一起发布。</p><p>当我们开始开发Harness CD Community Edition时，一个关键目标是以足够低的资源规格发布它，以便开发人员甚至可以在他们的个人笔记本电脑上下载、安装和使用该产品。我们希望拥有3 GB内存和2个CPU的任何人都能够毫无问题地使用该产品。</p><p>今天，在这篇文章的第1部分，我们将讨论如何在3 GB的内存限制内运行我们所有的服务。在第2部分中，我们将讨论如何实现2个CPU的其他目标，以及如何将整个产品启动时间保持在大约两分钟，以获得最佳的开发人员体验。</p><p>截至本文发布之日，我们可以在一台笔记本电脑上使用这些规格运行多达八个并发部署，没有任何问题！</p><h2>背景</h2><p>实际上，我们有一个基于微服务的架构。Harness CD Community Edition共附带12项服务:</p><ol role="list"><li>两个UI服务</li><li>基础设施服务(Mongo、Redis、NGINX)</li><li>用于处理机密、连接器、Git体验、身份验证的管理器服务[Java]</li><li>管道服务[Java]</li><li>委托代理服务</li><li>源代码管理服务(SCM) [Go]</li><li>日志服务[Go]</li><li>通知服务[Java]</li></ol><p>当我们开始这项工作时，这12个服务的总内存占用大约为12 GB。下图是"<em> docker stats" </em>命令的输出。</p><figure class="w-richtext-figure-type- "/><p>在这里，我们看到四个主要的基于Java的服务使用了总共12 GB内存中的大约11 GB。相比之下，其他八个服务消耗的内存要少得多。<br/> <br/>对于这八个服务，我们估计Mongo需要大约300 MB的内存，Redis需要128 MB的内存来满足预期的工作负载，其他六个服务大约可以容纳在300 MB的内存中。这为我们的其他四个主要服务留下了大约2.25 GB (3 - 0.75 GB)的内存。</p><p>这意味着我们必须将这四项服务的内存需求从当前每项服务平均2.75 GB降低到600 MB，才能实现我们的目标。下面几节详细介绍了我们是如何做到这一点的。</p><h2>内存分析</h2><p>在减少内存之前，我们首先必须了解这些服务是如何使用内存的。</p><p><em>注意，由于这四个服务都是基于Java的服务，我们在这里以NG Manager服务为例。其他三项服务也遵循了类似的程序。</em></p><p>Java提供了一种简单的可视化JVM内存使用的方法。我们可以启用本地内存跟踪(NMT)，它跟踪JVM的内部内存使用情况。可以使用以下命令行选项启用NMT:-XX:NativeMemoryTracking = summary</p><p>一旦启用了内存跟踪，我们就可以使用以下命令获得当前的JVM内存使用情况:jcmd<java_process_id>VM . native _ memory summary</java_process_id></p><p>下面是我们启动<em> NG管理器</em>服务时的内存配置文件:</p><p>Memory AreaDescriptionUsageHeapHolds持有Java对象。最大的内存块768 MB类元数据包含有关JVM已加载的类的信息180 MB线程线程堆栈(每个线程1 MB)600 MB代码由JIT编译器编译的代码放在这里50 MB GC算法用于其处理的内存80 MB内存用于JVM内部操作。这里分配了直接字节缓冲区200 MBSymbolConstants来自类文件50 MB</p><p>在分析这个概要文件时，我们获得了关于在哪里集中精力减少内存占用的有价值的见解。</p><ol role="list"><li>堆:它使用了大约40%的总内存。</li><li>线程:600个线程对于服务来说太高了。相反，该服务处于空闲状态，并且不提供任何负载。</li></ol><p>优化这两个区域可以获得高达1 GB的收益。因此，我们从首先关注它们开始。</p><h2>减少堆的使用</h2><p>为了减少堆内存，我们开始分析堆的使用情况。对于堆分析，堆转储是必需的。您可以使用以下命令从<a href="https://aws.amazon.com/cli/" target="_blank"> AWS命令行界面(CLI) </a>轻松生成堆转储:</p><p>jmap -dump:format=b，file=heapdump.hprof <java_process_id/></p><p>JProfiler、YourKit和JVM等工具可用于打开堆转储文件并执行分析。</p><p>当我们分析<em> NG管理器</em>服务的堆转储时，我们获得了以下见解:</p><ol role="list"><li>该服务在空闲状态下使用大约128 MB的堆(没有工作负载时)。</li><li>在运行部署时，堆的使用量会达到180 MB。</li><li>没有支配者对象。</li></ol><p>这表明我们给服务分配了过多的堆内存。它需要大约180 MB的内存，但是我们指定为768 MB。</p><p>最后，我们决定为服务分配最大256 MB的堆内存(空闲使用量的2倍)。</p><h2>减少螺纹</h2><p>线程转储是Java中获取进程内运行的每个线程的快照的标准方式。线程转储还捕获以下内容:<br/></p><ol role="list"><li>与每个线程关联的堆栈跟踪。</li><li>每个线程的状态(可运行/阻塞/等待)。</li></ol><p>可以使用以下命令从AWS CLI获取线程转储:</p><p>jstack<java_process_id/>的缩写形式</p><p>分析服务的线程转储揭示了大量线程的根本原因。</p><ol role="list"><li>我们使用Redis流在微服务之间进行事件驱动的通信。服务中的每个Redis生产者和消费者都有自己的Redisson客户端，每个Redisson客户端都配置了16个线程(见下图)。</li><li>一些executor服务是无限的。</li><li>有大量线程被配置为并行运行cron作业。</li></ol><figure class="w-richtext-figure-type- "/><p>为了解决这些问题，我们采取了以下措施:</p><ol role="list"><li>在生产者和消费者之间重用Redisson客户端。redis on客户端是线程安全的，所以重用redis on客户端不成问题。</li><li>绑定的执行者服务。</li><li>通过减少线程数量来降低运行cron作业时的并行度。</li></ol><p>做出这些更改后，我们将服务的线程数量减少到大约100个。</p><p>如您所见，在优化堆大小和减少线程数量后，我们减少了大约1 GB的内存。</p><p>作为这些变化的副作用，JVM内部内存使用量也从200 MB降到了80 MB。为了进一步降低内存使用，我们将重点转移到了GC和代码内存使用上。此外，我们不能优化类元数据的使用，主要是因为它包含关于JVM加载的类的信息，并且JVM只在应用程序需要时才加载一个类。</p><h2>减少垃圾收集和代码的使用</h2><p>为了减少GC内存的使用，我们将GC算法从G1GC切换到了串行GC。这一选择的主要原因是:</p><ol role="list"><li>选择一个并行GC算法，比如G1GC，它可以利用多个内核，在我们的例子中没有意义。我们拥有非常少的CPU周期(两个CPU运行所有12个服务)。</li><li>堆的大小很小，只有256 MB。因此，我们可以牺牲G1GC从堆中收集垃圾的能力，把它分成几个区域并并行处理它们。</li></ol><p>我们还禁用了分层编译(-XX:-TieredCompilation)，因为我们受到内存的严重限制。在我们的性能测试中，我们没有发现应用程序的任何性能下降的副作用。所以，我们坚持了这个选择。</p><h2>决赛成绩</h2><p>在做了所有这些改变之后，我们可以将<em> NG管理器</em>服务的内存使用降低到大约600 MB。</p><p>内存区域使用eap256 MBClass元数据160 MB threads 90 MB code 7 mbg C1 MB internal 40 MB symbol 40 MB</p><figure class="w-richtext-figure-type- "/><p>在对其他三个服务进行类似的分析和更改后，我们还可以显著降低它们的内存使用量。</p><p>服务内存使用管道660 MBCG管理器780 MBNG管理器MB平台244 MB</p><h2>结论</h2><p>我们希望您喜欢我们的旅程。但我们不会就此止步。我们已经开始努力进一步降低内存需求，为开发者提供最佳体验。</p><p>不要等着尝试利用CD社区版。Harness CD社区版可以在GitHub 上找到<a href="https://github.com/harness/harness-cd-community" target="_blank">。</a></p><p>请继续关注关于优化CPU使用和产品启动时间的第2部分！</p></div></div>    
</body>
</html>