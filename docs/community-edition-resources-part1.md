# 线束光盘社区版的资源优化-第 1 部分|线束

> 原文：<https://www.harness.io/blog/community-edition-resources-part1>

了解我们如何在 3 GB 内存和 2 个 CPU 的资源要求下提供 Harness CD Community Edition。

最近，Harness 推出了它的 CD 社区版。这是 Harness CD 的一个永久免费、有源代码的本地版本。加入我们的旅程，了解我们如何设法将 Harness CD Community Edition 与 3 GB 内存和 2 个 CPU 的资源要求一起发布。

当我们开始开发 Harness CD Community Edition 时，一个关键目标是以足够低的资源规格发布它，以便开发人员甚至可以在他们的个人笔记本电脑上下载、安装和使用该产品。我们希望拥有 3 GB 内存和 2 个 CPU 的任何人都能够毫无问题地使用该产品。

今天，在这篇文章的第 1 部分，我们将讨论如何在 3 GB 的内存限制内运行我们所有的服务。在第 2 部分中，我们将讨论如何实现 2 个 CPU 的其他目标，以及如何将整个产品启动时间保持在大约两分钟，以获得最佳的开发人员体验。

截至本文发布之日，我们可以在一台笔记本电脑上使用这些规格运行多达八个并发部署，没有任何问题！

## 背景

实际上，我们有一个基于微服务的架构。Harness CD Community Edition 共附带 12 项服务:

1.  两个 UI 服务
2.  基础设施服务(Mongo、Redis、NGINX)
3.  用于处理机密、连接器、Git 体验、身份验证的管理器服务[Java]
4.  管道服务[Java]
5.  委托代理服务
6.  源代码管理服务(SCM) [Go]
7.  日志服务[Go]
8.  通知服务[Java]

当我们开始这项工作时，这 12 个服务的总内存占用大约为 12 GB。下图是" *docker stats"* 命令的输出。

在这里，我们看到四个主要的基于 Java 的服务使用了总共 12 GB 内存中的大约 11 GB。相比之下，其他八个服务消耗的内存要少得多。

对于这八个服务，我们估计 Mongo 需要大约 300 MB 的内存，Redis 需要 128 MB 的内存来满足预期的工作负载，其他六个服务大约可以容纳在 300 MB 的内存中。这为我们的其他四个主要服务留下了大约 2.25 GB (3 - 0.75 GB)的内存。

这意味着我们必须将这四项服务的内存需求从当前每项服务平均 2.75 GB 降低到 600 MB，才能实现我们的目标。下面几节详细介绍了我们是如何做到这一点的。

## 内存分析

在减少内存之前，我们首先必须了解这些服务是如何使用内存的。

*注意，由于这四个服务都是基于 Java 的服务，我们在这里以 NG Manager 服务为例。其他三项服务也遵循了类似的程序。*

Java 提供了一种简单的可视化 JVM 内存使用的方法。我们可以启用本地内存跟踪(NMT)，它跟踪 JVM 的内部内存使用情况。可以使用以下命令行选项启用 NMT:-XX:NativeMemoryTracking = summary

一旦启用了内存跟踪，我们就可以使用以下命令获得当前的 JVM 内存使用情况:jcmd<java_process_id>VM . native _ memory summary</java_process_id>

下面是我们启动 *NG 管理器*服务时的内存配置文件:

Memory AreaDescriptionUsageHeapHolds 持有 Java 对象。最大的内存块 768 MB 类元数据包含有关 JVM 已加载的类的信息 180 MB 线程线程堆栈(每个线程 1 MB)600 MB 代码由 JIT 编译器编译的代码放在这里 50 MB GC 算法用于其处理的内存 80 MB 内存用于 JVM 内部操作。这里分配了直接字节缓冲区 200 MBSymbolConstants 来自类文件 50 MB

在分析这个概要文件时，我们获得了关于在哪里集中精力减少内存占用的有价值的见解。

1.  堆:它使用了大约 40%的总内存。
2.  线程:600 个线程对于服务来说太高了。相反，该服务处于空闲状态，并且不提供任何负载。

优化这两个区域可以获得高达 1 GB 的收益。因此，我们从首先关注它们开始。

## 减少堆的使用

为了减少堆内存，我们开始分析堆的使用情况。对于堆分析，堆转储是必需的。您可以使用以下命令从 [AWS 命令行界面(CLI)](https://aws.amazon.com/cli/) 轻松生成堆转储:

jmap -dump:format=b，file=heapdump.hprof<java_process_id></java_process_id>

JProfiler、YourKit 和 JVM 等工具可用于打开堆转储文件并执行分析。

当我们分析 *NG 管理器*服务的堆转储时，我们获得了以下见解:

1.  该服务在空闲状态下使用大约 128 MB 的堆(没有工作负载时)。
2.  在运行部署时，堆的使用量会达到 180 MB。
3.  没有支配者对象。

这表明我们给服务分配了过多的堆内存。它需要大约 180 MB 的内存，但是我们指定为 768 MB。

最后，我们决定为服务分配最大 256 MB 的堆内存(空闲使用量的 2 倍)。

## 减少螺纹

线程转储是 Java 中获取进程内运行的每个线程的快照的标准方式。线程转储还捕获以下内容:

1.  与每个线程关联的堆栈跟踪。
2.  每个线程的状态(可运行/阻塞/等待)。

可以使用以下命令从 AWS CLI 获取线程转储:

jstack<java_process_id>的缩写形式</java_process_id>

分析服务的线程转储揭示了大量线程的根本原因。

1.  我们使用 Redis 流在微服务之间进行事件驱动的通信。服务中的每个 Redis 生产者和消费者都有自己的 Redisson 客户端，每个 Redisson 客户端都配置了 16 个线程(见下图)。
2.  一些 executor 服务是无限的。
3.  有大量线程被配置为并行运行 cron 作业。

为了解决这些问题，我们采取了以下措施:

1.  在生产者和消费者之间重用 Redisson 客户端。redis on 客户端是线程安全的，所以重用 redis on 客户端不成问题。
2.  绑定的执行者服务。
3.  通过减少线程数量来降低运行 cron 作业时的并行度。

做出这些更改后，我们将服务的线程数量减少到大约 100 个。

如您所见，在优化堆大小和减少线程数量后，我们减少了大约 1 GB 的内存。

作为这些变化的副作用，JVM 内部内存使用量也从 200 MB 降到了 80 MB。为了进一步降低内存使用，我们将重点转移到了 GC 和代码内存使用上。此外，我们不能优化类元数据的使用，主要是因为它包含关于 JVM 加载的类的信息，并且 JVM 只在应用程序需要时才加载一个类。

## 减少垃圾收集和代码的使用

为了减少 GC 内存的使用，我们将 GC 算法从 G1GC 切换到了串行 GC。这一选择的主要原因是:

1.  选择一个并行 GC 算法，比如 G1GC，它可以利用多个内核，在我们的例子中没有意义。我们拥有非常少的 CPU 周期(两个 CPU 运行所有 12 个服务)。
2.  堆的大小很小，只有 256 MB。因此，我们可以牺牲 G1GC 从堆中收集垃圾的能力，把它分成几个区域并并行处理它们。

我们还禁用了分层编译(-XX:-TieredCompilation)，因为我们受到内存的严重限制。在我们的性能测试中，我们没有发现应用程序的任何性能下降的副作用。所以，我们坚持了这个选择。

## 决赛成绩

在做了所有这些改变之后，我们可以将 *NG 管理器*服务的内存使用降低到大约 600 MB。

内存区域使用 eap256 MBClass 元数据 160 MB threads 90 MB code 7 mbg C1 MB internal 40 MB symbol 40 MB

在对其他三个服务进行类似的分析和更改后，我们还可以显著降低它们的内存使用量。

服务内存使用管道 660 MBCG 管理器 780 MBNG 管理器 MB 平台 244 MB

## 结论

我们希望您喜欢我们的旅程。但我们不会就此止步。我们已经开始努力进一步降低内存需求，为开发者提供最佳体验。

不要等着尝试利用 CD 社区版。Harness CD 社区版可以在 GitHub 上找到[。](https://github.com/harness/harness-cd-community)

请继续关注关于优化 CPU 使用和产品启动时间的第 2 部分！